<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8" /><script type="text/javascript">window.NREUM||(NREUM={}),__nr_require=function(e,n,t){function r(t){if(!n[t]){var o=n[t]={exports:{}};e[t][0].call(o.exports,function(n){var o=e[t][1][n];return r(o||n)},o,o.exports)}return n[t].exports}if("function"==typeof __nr_require)return __nr_require;for(var o=0;o<t.length;o++)r(t[o]);return r}({1:[function(e,n,t){function r(){}function o(e,n,t){return function(){return i(e,[c.now()].concat(u(arguments)),n?null:this,t),n?void 0:this}}var i=e("handle"),a=e(2),u=e(3),f=e("ee").get("tracer"),c=e("loader"),s=NREUM;"undefined"==typeof window.newrelic&&(newrelic=s);var p=["setPageViewName","setCustomAttribute","setErrorHandler","finished","addToTrace","inlineHit","addRelease"],d="api-",l=d+"ixn-";a(p,function(e,n){s[n]=o(d+n,!0,"api")}),s.addPageAction=o(d+"addPageAction",!0),s.setCurrentRouteName=o(d+"routeName",!0),n.exports=newrelic,s.interaction=function(){return(new r).get()};var m=r.prototype={createTracer:function(e,n){var t={},r=this,o="function"==typeof n;return i(l+"tracer",[c.now(),e,t],r),function(){if(f.emit((o?"":"no-")+"fn-start",[c.now(),r,o],t),o)try{return n.apply(this,arguments)}finally{f.emit("fn-end",[c.now()],t)}}}};a("setName,setAttribute,save,ignore,onEnd,getContext,end,get".split(","),function(e,n){m[n]=o(l+n)}),newrelic.noticeError=function(e){"string"==typeof e&&(e=new Error(e)),i("err",[e,c.now()])}},{}],2:[function(e,n,t){function r(e,n){var t=[],r="",i=0;for(r in e)o.call(e,r)&&(t[i]=n(r,e[r]),i+=1);return t}var o=Object.prototype.hasOwnProperty;n.exports=r},{}],3:[function(e,n,t){function r(e,n,t){n||(n=0),"undefined"==typeof t&&(t=e?e.length:0);for(var r=-1,o=t-n||0,i=Array(o<0?0:o);++r<o;)i[r]=e[n+r];return i}n.exports=r},{}],4:[function(e,n,t){n.exports={exists:"undefined"!=typeof window.performance&&window.performance.timing&&"undefined"!=typeof window.performance.timing.navigationStart}},{}],ee:[function(e,n,t){function r(){}function o(e){function n(e){return e&&e instanceof r?e:e?f(e,u,i):i()}function t(t,r,o,i){if(!d.aborted||i){e&&e(t,r,o);for(var a=n(o),u=m(t),f=u.length,c=0;c<f;c++)u[c].apply(a,r);var p=s[y[t]];return p&&p.push([b,t,r,a]),a}}function l(e,n){v[e]=m(e).concat(n)}function m(e){return v[e]||[]}function w(e){return p[e]=p[e]||o(t)}function g(e,n){c(e,function(e,t){n=n||"feature",y[t]=n,n in s||(s[n]=[])})}var v={},y={},b={on:l,emit:t,get:w,listeners:m,context:n,buffer:g,abort:a,aborted:!1};return b}function i(){return new r}function a(){(s.api||s.feature)&&(d.aborted=!0,s=d.backlog={})}var u="nr@context",f=e("gos"),c=e(2),s={},p={},d=n.exports=o();d.backlog=s},{}],gos:[function(e,n,t){function r(e,n,t){if(o.call(e,n))return e[n];var r=t();if(Object.defineProperty&&Object.keys)try{return Object.defineProperty(e,n,{value:r,writable:!0,enumerable:!1}),r}catch(i){}return e[n]=r,r}var o=Object.prototype.hasOwnProperty;n.exports=r},{}],handle:[function(e,n,t){function r(e,n,t,r){o.buffer([e],r),o.emit(e,n,t)}var o=e("ee").get("handle");n.exports=r,r.ee=o},{}],id:[function(e,n,t){function r(e){var n=typeof e;return!e||"object"!==n&&"function"!==n?-1:e===window?0:a(e,i,function(){return o++})}var o=1,i="nr@id",a=e("gos");n.exports=r},{}],loader:[function(e,n,t){function r(){if(!x++){var e=h.info=NREUM.info,n=d.getElementsByTagName("script")[0];if(setTimeout(s.abort,3e4),!(e&&e.licenseKey&&e.applicationID&&n))return s.abort();c(y,function(n,t){e[n]||(e[n]=t)}),f("mark",["onload",a()+h.offset],null,"api");var t=d.createElement("script");t.src="https://"+e.agent,n.parentNode.insertBefore(t,n)}}function o(){"complete"===d.readyState&&i()}function i(){f("mark",["domContent",a()+h.offset],null,"api")}function a(){return E.exists&&performance.now?Math.round(performance.now()):(u=Math.max((new Date).getTime(),u))-h.offset}var u=(new Date).getTime(),f=e("handle"),c=e(2),s=e("ee"),p=window,d=p.document,l="addEventListener",m="attachEvent",w=p.XMLHttpRequest,g=w&&w.prototype;NREUM.o={ST:setTimeout,SI:p.setImmediate,CT:clearTimeout,XHR:w,REQ:p.Request,EV:p.Event,PR:p.Promise,MO:p.MutationObserver};var v=""+location,y={beacon:"bam.nr-data.net",errorBeacon:"bam.nr-data.net",agent:"js-agent.newrelic.com/nr-1039.min.js"},b=w&&g&&g[l]&&!/CriOS/.test(navigator.userAgent),h=n.exports={offset:u,now:a,origin:v,features:{},xhrWrappable:b};e(1),d[l]?(d[l]("DOMContentLoaded",i,!1),p[l]("load",r,!1)):(d[m]("onreadystatechange",o),p[m]("onload",r)),f("mark",["firstbyte",u],null,"api");var x=0,E=e(4)},{}]},{},["loader"]);</script><script type="text/javascript">window.NREUM||(NREUM={});NREUM.info={"beacon":"bam.nr-data.net","queueTime":0,"licenseKey":"cd1101aa94","agent":"","transactionName":"bgcDMkBZW0NZBkdcWVdNJxNcW0FZVwscV1dKB08CV1taQlkRXEdFAz0WFFNIRVVcOkVcU049BxNcWw==","applicationID":"6369297","errorBeacon":"bam.nr-data.net","applicationTime":256}</script>
    <title>Pathways4Life: Crowdsourcing Pathway Modeling from Published Figures | Thinklab</title>
	  <link rel="shortcut icon" href="https://think-lab.s3.amazonaws.com/s/img/favicon.ico" type="image/x-icon" />
    <meta property="og:image" content="https://think-lab.s3.amazonaws.com/s/img" />
    
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" integrity="sha384-1q8mTJOASx8j1Au+a5WDVnPi2lkFfwwEAa8hDDdjZlpLegxhjVME1fgjWPGmkzs7" crossorigin="anonymous">
  <link rel="stylesheet" href="https://code.jquery.com/ui/1.11.4/themes/smoothness/jquery-ui.css" />

<link rel="stylesheet" href="https://think-lab.s3.amazonaws.com/s/highlightjs/github.css" />
<link rel="stylesheet" href="https://think-lab.s3.amazonaws.com/s/css/css.css" />
<link rel="stylesheet" href="https://think-lab.s3.amazonaws.com/s/css/bootstrap-multiselect.css" />
<!-- <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'> -->

    
    
    
    
    


	<!-- GoogleAnalytics -->
	<script>
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','//www.google-analytics.com/analytics.js','ga');

	ga('create', 'UA-48550230-1', 'thinklab.com');
	ga('require', 'displayfeatures');
	
	ga('send', 'pageview');

	</script>






</head>
<body >
  <form id="form_action" method="post">
    <input type='hidden' name='csrfmiddlewaretoken' value='pQNiZQX4gF2vyFeXrnbUl9EmuTWA5MG1' />
</form>

	

	<div class="body-wrapper">

		


<!-- <div style="height:5px;background-image:url('https://think-lab.s3.amazonaws.com/s/img/top-bg.gif');"></div> -->
<div class="gradient">
	<div class="main-centered-div header-container">

<table class="full-width" style="height:60px;"><tr>
	<td style="width:200px;vertical-align:middle;padding-bottom:15px;">
		<a href="/" style="font-weight:normal;color:#212121;"><img src="https://think-lab.s3.amazonaws.com/s/img/logo.png" width="160"></a>
	</td><td style="vertical-align:middle;text-align:left;padding-top:6px;width:100%;padding-left:40px;">
		<ul id="not-search-block"  class="main-header">


			


				<li class="dropdown">
					<a class="dropdown-toggle" data-toggle="dropdown" href="#">Browse <span class="caret"></span></a>
					<ul class="dropdown-menu" role="menu">
						<li><a href="/proposals">Proposals</a></li>
						<li><a href="/projects">Projects</a></li>
						<li><a href="/publications">Article discussion</a></li>
						<li><a href="/discussion">All discussion</a></li>
					</ul>
				</li>
				<li><a href="/leaderboard">Leaderboard</a></li>

			
				<li><a href="/about">About</a></li>
			


		</ul>

    <ul id="search-block" class="main-header" style="display: none; margin-right: 30px;">
        <li class="full-width">
            <form method="get" action="/search">
			<div class="input-group full-width">
				<input name="q" type="text" class="form-control search-input">
			</div>
            </form>
        </li>
    </ul>

	</td><td style="vertical-align:middle;text-align:right;padding-top:6px; min-width: 1%; white-space: nowrap;">
		<ul class="main-header pull-right" style="padding-left: 0;">
    		
                <li><a href="/login" style="color:silver;">Login</a></li>
                <li style="margin-right:0"><a class="btn btn-default btn-sm btn-upper" href="/signup">Join</a></li>
            

        </ul>
    </ul>
	</td>
</tr></table>

<!-- <div id="global-search-div" class="main-centered-div" style="display:none;">
	<input type="text" value="" class="thread-search form-control" placeholder="Search" data-action="/discussion">
</div> -->

	</div>
</div>

		
	  



	  


		

		<div class="dark-head"><div class="main-centered-div">

			

	


	<div class="header-object-type"></div>

	
			<a class="btn btn-lg follow-link btn-follow pull-right active tt-top" style="margin-left:20px;" title="Follow project discussion" data-id="8" data-ctype="project" data-is-following="false">&nbsp;</a>
	

	<h2 class="header-object-name">Pathways4Life: Crowdsourcing Pathway Modeling from Published Figures</h2>

	
	<div class="project-team">
		<a href='/u/alexanderpico'>Alexander Pico</a>
		 <!-- &nbsp; &nbsp; doi:<a href="https://doi.org/10.15363/thinklab.8">10.15363/thinklab.8</a> -->
	</div>
	

		<!-- <div class="project-team">
			<b>DOI:</b> <a href="https://doi.org/10.15363/thinklab.8">10.15363/thinklab.8</a>
		</div> -->

		
			<div class="project-team">
			<div class="proj-funding">
				<b>Funding opportunity:</b>
				
					<a href="/funding/3/proposals">Big Data to Knowledge (BD2K) Advancing Biomedical Science Using Crowdsourcing and Interactive Digital Media (UH2)</a>
				
			</div>
			</div>
		

	<div class="project-team">
		
		<div class="proposal-status proposal-status-lg proposal-status-awaiting-funder">Awaiting Funder</div>
		
		
		
	</div>




	


	<!-- <div class="project-team">
		
	</div> -->

    <ul class="nav nav-tabs project-nav" style="margin-top:25px;">

		

		

    

		
	    
	        <li class="active"><a href="/p/pathways4life/proposal">Proposal</a></li>
	    
	    
		

		<li><a href="/p/pathways4life/discussion"><span class="nav-fig">10</span>Discussions</a></li>

		
			<li><a href="/p/pathways4life/leaderboard"><span class="nav-fig">5</span>Reviewers</a></li>
		

		

		


        <li class="dropdown">
            <a class="dropdown-toggle" data-toggle="dropdown"><span style="opacity:0.5">More <span class="caret"></span></span></a>
            <ul class="dropdown-menu" style="font-size:13px;background-color:#819ec8">

				

				

            <li><a href="/p/pathways4life/followers"><span class="nav-fig">5</span>Followers</a></li>
            </ul>
        </li>

			

    </ul>



		</div></div>
		
		<div class="main-centered-div" style="margin-top:40px;padding-bottom:340px">
	    
	<h2 class="page-title">Proposal revision comparison</h2>

	<p>
		You are comparing versions dated
		<a href="/doc/3/revisions/e9c38e5a4ca329d06d49cf79129da34d8be13900">Sept. 8, 2015, 10:15 a.m.</a>
		and <a href="/doc/3/revisions/7fd6215967116c78cc43e5764e0a9a362e3b2866">Jan. 13, 2016, 10:25 a.m.</a>
	</p>

	<hr>
	
    <h3>Content</h3>
        <table class="full-width diff-table">
        

            
            
            
                <tr class="repo-line" height="25px">
                    <td style="width: 35px;" colspan="4"></td>
            
            
            
            
            
            

            
            

            
            
        

            
            
            
            
            
            
            
            

            
            

            
            
                    <td class="diff-marker" style="width: 35px;" colspan="3">+</td>
                    <td class="add-line diff-context" style="width: 45%;"># Abstract
<br>
<br>A wealth of novel pathway information is trapped in published figures. This information, if properly modeled would be immensely useful for analyzing and interpreting large-scale omics datasets. Aligned with the broader BD2K initiative, this proposal sets out to transform the wealth of information currently embedded in countless figure images (big data) into pathway models amenable to analysis and research (knowledge). Computational approaches alone have failed to fully automate the extraction of this knowledge, which is no surprise given the wide diversity among images. Likewise, human efforts have fallen short, both at the level of internal curation teams (it’s a massively distributed problem, after all) and at the level of individual researchers who choose PowerPoint and Illustrator over the freely available pathway modeling standards and tools. This challenge is particularly well suited for a computer-assisted, human-crowdsourced solution. We propose to develop the Pathways4Life platform, which combines image processing, text recognition, and cutting-edge pathway modeling software together with scalable infrastructure for content management and tunable game mechanics to facilitate the rapid modeling of pathway images through human crowdsourced tasks.
<br>
</td>
                </tr>

            
        

            
            
            
            
            
            
                <tr class="repo-line" height="25px">
                    <td class="" style="width: 35px;" colspan="3"></td>
                    <td class="diff-context" style="width: 45%;"># RESEARCH STRATEGY
<br>## Significance
<br>Pathway information is immensely useful for analyzing and interpreting large-scale omics data [@10.1038/ng1109 @10.1371/journal.pbio.1000472]. Pathway analysis software for general use was developed to meet the challenge of analyzing and interpreting high-throughput transcriptomics data. After the commercialization of microarrays, pathway analysis tools such as GenMAPP [@10.1038/ng0502-19], Pathway Tools [@10.1093/bioinformatics/18.suppl_1.s225], and Ingenuity (www.qiagen.com/ingenuity) proliferated. These tools ultimately rely on knowledge bases of pathway models. In this grant, we stress the distinction between pathway figures, which are drawn purely for illustration purposes in a graphical file format (e.g., jpg, gif, png) and pathway models, which contain standard identifiers and semantics that can be mapped to external resources in a structured file format (e.g., xml, owl, json). Aligned with the broader BD2K initiative, this proposal sets out to transform the wealth of information currently trapped in countless figure images (big data) into properly modeled pathways amenable to analysis and research (knowledge).
<br>
</td>

                    <td class="" style="width: 35px;" colspan="3"></td>
                    <td class="diff-context" style="width: 45%;"># RESEARCH STRATEGY
<br>## Significance
<br>Pathway information is immensely useful for analyzing and interpreting large-scale omics data [@10.1038/ng1109 @10.1371/journal.pbio.1000472]. Pathway analysis software for general use was developed to meet the challenge of analyzing and interpreting high-throughput transcriptomics data. After the commercialization of microarrays, pathway analysis tools such as GenMAPP [@10.1038/ng0502-19], Pathway Tools [@10.1093/bioinformatics/18.suppl_1.s225], and Ingenuity (www.qiagen.com/ingenuity) proliferated. These tools ultimately rely on knowledge bases of pathway models. In this grant, we stress the distinction between pathway figures, which are drawn purely for illustration purposes in a graphical file format (e.g., jpg, gif, png) and pathway models, which contain standard identifiers and semantics that can be mapped to external resources in a structured file format (e.g., xml, owl, json). Aligned with the broader BD2K initiative, this proposal sets out to transform the wealth of information currently trapped in countless figure images (big data) into properly modeled pathways amenable to analysis and research (knowledge).
<br>
</td>
                </tr>
            
            
            

            
            

            
            
        

            
            
            
            
            
            
            
            

            
            
                <tr class="repo-line">
                    <td class="diff-marker" style="width: 35px;" colspan="3">-</td>
                    <td class="sub-line diff-context" style="width: 45%;">Eight years ago, we conceived of WikiPathways to bring a new approach to the task of collecting, curating and distributing pathway models [@10.1371/journal.pbio.0060184 @10.1093/nar/gkr1074]. Like other pathway knowledgebases, such as KEGG [@10.1093/nar/28.1.27] and Reactome [@10.1093/nar/gkq1018], WikiPathways manages a wide range of canonical metabolic, regulatory, and signaling pathways ([Figure](#VennHuman)). The pathway creation and editing tools we use to centrally curate content, however, are the same ones we embedded into the MediaWiki platform and make available to anyone at wikipathways.org. Crowdsourcing the tasks involved in curation enables WikiPathways to manage more updates, tap into more diverse domain experts, and service specialized research communities ([Table](#CurationActivity)). WikiPathways hosts any pathway model that is of interest to any individual researcher. 
</td>
            

            
            
        

            
            
            
                <tr class="repo-line" height="25px">
                    <td style="width: 35px;" colspan="4"></td>
            
            
            
            
            
            

            
            

            
            
        

            
            
            
            
            
            
            
            

            
            

            
            
                    <td class="diff-marker" style="width: 35px;" colspan="3">+</td>
                    <td class="add-line diff-context" style="width: 45%;">Eight years ago, we conceived of WikiPathways to bring a new approach to the task of collecting, curating and distributing pathway models [@10.1371/journal.pbio.0060184 @10.1093/nar/gkr1074]. Like other pathway knowledgebases, such as KEGG [@10.1093/nar/28.1.27] and Reactome [@10.1093/nar/gkq1018], WikiPathways manages a wide range of canonical metabolic, regulatory, and signaling pathways ([Figure {n}](#VennHuman)). The pathway creation and editing tools we use to centrally curate content, however, are the same ones we embedded into the MediaWiki platform and make available to anyone at wikipathways.org. Crowdsourcing the tasks involved in curation enables WikiPathways to manage more updates, tap into more diverse domain experts, and service specialized research communities ([Table {n}](#CurationActivity)). WikiPathways hosts any pathway model that is of interest to any individual researcher. 
</td>
                </tr>

            
        

            
            
            
            
            
            
                <tr class="repo-line" height="25px">
                    <td class="" style="width: 35px;" colspan="3"></td>
                    <td class="diff-context" style="width: 45%;">
<br>[:figure](VennHuman)
<br>
<br>[:table](CurationActivity)
<br>
</td>

                    <td class="" style="width: 35px;" colspan="3"></td>
                    <td class="diff-context" style="width: 45%;">
<br>[:figure](VennHuman)
<br>
<br>[:table](CurationActivity)
<br>
</td>
                </tr>
            
            
            

            
            

            
            
        

            
            
            
            
            
            
            
            

            
            
                <tr class="repo-line">
                    <td class="diff-marker" style="width: 35px;" colspan="3">-</td>
                    <td class="sub-line diff-context" style="width: 45%;">We are collaborating with Reactome to convert and host their human content for crowdsourced curation and distribution. Thus, despite the yellow portion in [Figure](#VennHuman) (the KEGG-only content that is not programmatically accessible without a license), WikiPathways is an ideal resource from which to launch a new, ambitious crowdsourcing initiative. Despite our efforts and the tremendous effort by all pathway knowledge bases over the past decade, most pathway information is still published solely as static, arbitrarily drawn images—isolated, inert representations of knowledge that cannot readily be reused or remixed in future studies.
</td>
            

            
            
        

            
            
            
                <tr class="repo-line" height="25px">
                    <td style="width: 35px;" colspan="4"></td>
            
            
            
            
            
            

            
            

            
            
        

            
            
            
            
            
            
            
            

            
            

            
            
                    <td class="diff-marker" style="width: 35px;" colspan="3">+</td>
                    <td class="add-line diff-context" style="width: 45%;">We are collaborating with Reactome to convert and host their human content for crowdsourced curation and distribution. Thus, despite the yellow portion in [Figure {n}](#VennHuman) (the KEGG-only content that is not programmatically accessible without a license), WikiPathways is an ideal resource from which to launch a new, ambitious crowdsourcing initiative. Despite our efforts and the tremendous effort by all pathway knowledge bases over the past decade, most pathway information is still published solely as static, arbitrarily drawn images—isolated, inert representations of knowledge that cannot readily be reused or remixed in future studies.
</td>
                </tr>

            
        

            
            
            
            
            
            
                <tr class="repo-line" height="25px">
                    <td class="" style="width: 35px;" colspan="3"></td>
                    <td class="diff-context" style="width: 45%;">
</td>

                    <td class="" style="width: 35px;" colspan="3"></td>
                    <td class="diff-context" style="width: 45%;">
</td>
                </tr>
            
            
            

            
            

            
            
        

            
            
            
            
            
            
            
            

            
            
                <tr class="repo-line">
                    <td class="diff-marker" style="width: 35px;" colspan="3">-</td>
                    <td class="sub-line diff-context" style="width: 45%;">A survey of ~4000 published pathway figures highlights the challenges we propose to address. A PubMed Central (PMC) image search using the keyword “signaling pathway” generates over 40,000 results. Visual inspection of the first 5000 results, from publications spanning 2000–2015, revealed that 3985 (79.7%) contain a pathway image; the remainder contained only the word “pathway” in their captions. We then performed optical character recognition (OCR) using two parallel approaches: Adobe Acrobat Text Recognition (www.adobe.com) and Google’s Tesseract (code.google.com/p/tesseract-ocr). We cross-referenced the extracted text results against all known HGNC human gene symbols [@10.1093/nar/gku1071], including aliases and prior symbols, to assess the potential of these images to inform the curation of human and orthologous pathways. Acrobat and Tesseract each extracted over ~2300 HGNC symbols; ~730 (~32%) contained new information, human genes, and orthologs not captured in any pathway for any species at WikiPathways. Further, these approaches found significantly different sets of symbols—each with its own uniquely trained OCR method—such that the combined results provide greater extraction counts across all categories: 3187 HGNC symbols in total and 1087 (34%) new to WikiPathways ([Figure](#HGNC), green).
</td>
            

            
            
        

            
            
            
                <tr class="repo-line" height="25px">
                    <td style="width: 35px;" colspan="4"></td>
            
            
            
            
            
            

            
            

            
            
        

            
            
            
            
            
            
            
            

            
            

            
            
                    <td class="diff-marker" style="width: 35px;" colspan="3">+</td>
                    <td class="add-line diff-context" style="width: 45%;">A survey of ~4000 published pathway figures highlights the challenges we propose to address. A PubMed Central (PMC) image search using the keyword “signaling pathway” generates over 40,000 results. Visual inspection of the first 5000 results, from publications spanning 2000–2015, revealed that 3985 (79.7%) contain a pathway image; the remainder contained only the word “pathway” in their captions. We then performed optical character recognition (OCR) using two parallel approaches: Adobe Acrobat Text Recognition (www.adobe.com) and Google’s Tesseract (code.google.com/p/tesseract-ocr). We cross-referenced the extracted text results against all known HGNC human gene symbols [@10.1093/nar/gku1071], including aliases and prior symbols, to assess the potential of these images to inform the curation of human and orthologous pathways. Acrobat and Tesseract each extracted over ~2300 HGNC symbols; ~730 (~32%) contained new information, human genes, and orthologs not captured in any pathway for any species at WikiPathways. Further, these approaches found significantly different sets of symbols—each with its own uniquely trained OCR method—such that the combined results provide greater extraction counts across all categories: 3187 HGNC symbols in total and 1087 (34%) new to WikiPathways ([Figure {n}, green](#HGNC)).
</td>
                </tr>

            
        

            
            
            
            
            
            
                <tr class="repo-line" height="25px">
                    <td class="" style="width: 35px;" colspan="3"></td>
                    <td class="diff-context" style="width: 45%;">
<br>[:figure](HGNC)
<br>
</td>

                    <td class="" style="width: 35px;" colspan="3"></td>
                    <td class="diff-context" style="width: 45%;">
<br>[:figure](HGNC)
<br>
</td>
                </tr>
            
            
            

            
            

            
            
        

            
            
            
            
            
            
            
            

            
            
                <tr class="repo-line">
                    <td class="diff-marker" style="width: 35px;" colspan="3">-</td>
                    <td class="sub-line diff-context" style="width: 45%;">Several caveats to this survey suggest that even more new pathway information can be extracted from published images. (1) Another 38% of extracted symbols ([Figure](#HGNC), red) are found only on nonhuman pathways and thus may still represent novel human pathway content. (2) The OCR methods were used “out-of-the-box” and not trained on pathway images, and the images were not pre-processed. Thus there is a significant opportunity to increase total extraction counts. (3) Since we considered only 5000 of 40,000 results from only a single pathway-related search term, the search result space is much greater. (4) This survey ignored the interactions shown in the images. Thus, even the 28% of symbols that overlap with current human pathways ([Figure](#HGNC), blue) may provide new interaction content. These caveats far outweigh the potential for false positives in this survey. (1) About 5.5% of extracted symbols are dictionary words that might not represent human genes in a pathway (e.g., BIG, CELL, HOOK, MASS). (2) Some occurrences of extracted symbols might be peripheral to the image and not part of the pathway. (3) Each OCR method has an inherent false-positive rate of &lt;5% [@ 10.1109/icdar.2007.4376991], which is only partially mitigated by the narrow focus on HGNC cross-referenced hits.
</td>
            

            
            
        

            
            
            
                <tr class="repo-line" height="25px">
                    <td style="width: 35px;" colspan="4"></td>
            
            
            
            
            
            

            
            

            
            
        

            
            
            
            
            
            
            
            

            
            

            
            
                    <td class="diff-marker" style="width: 35px;" colspan="3">+</td>
                    <td class="add-line diff-context" style="width: 45%;">Several caveats to this survey suggest that even more new pathway information can be extracted from published images. (1) Another 38% of extracted symbols ([Figure {n}, red](#HGNC)) are found only on nonhuman pathways and thus may still represent novel human pathway content. (2) The OCR methods were used “out-of-the-box” and not trained on pathway images, and the images were not pre-processed. Thus there is a significant opportunity to increase total extraction counts. (3) Since we considered only 5000 of 40,000 results from only a single pathway-related search term, the search result space is much greater. (4) This survey ignored the interactions shown in the images. Thus, even the 28% of symbols that overlap with current human pathways ([Figure {n}, blue](#HGNC)) may provide new interaction content. These caveats far outweigh the potential for false positives in this survey. (1) About 5.5% of extracted symbols are dictionary words that might not represent human genes in a pathway (e.g., BIG, CELL, HOOK, MASS). (2) Some occurrences of extracted symbols might be peripheral to the image and not part of the pathway. (3) Each OCR method has an inherent false-positive rate of &lt;5% [@ 10.1109/icdar.2007.4376991], which is only partially mitigated by the narrow focus on HGNC cross-referenced hits.
</td>
                </tr>

            
        

            
            
            
            
            
            
                <tr class="repo-line" height="25px">
                    <td class="" style="width: 35px;" colspan="3"></td>
                    <td class="diff-context" style="width: 45%;">
<br>It is difficult to estimate the total number of new human gene symbols in the entire corpus of published pathway images. The proportion of new genes must plateau—new results giving diminishing returns—as the total number of known genes in pathways is approached. But for perspective, even if we were to only consider the first 3985 images and conservatively estimate the average number of genes per pathway image to be 7, and generously estimate the false positive rate to be as high as 20%, then at a proportion of 34% we would expect ~6100 new genes. **This would almost double the count of unique human genes at WikiPathways today—the equivalent of 6 years of work at current crowdsourcing rates.** The effort would also greatly expand upon the interactions and biological context for practically all the genes in WikiPathways. As detailed in our data sharing plan, all of this new knowledge will be available in multiple formats, including BioPAX and RDF, and distributed to a wide range of independent resources through channels already established by the WikiPathways project, including Pathway Commons [@10.1093/nar/gkq1039], [NCBI](www.ncbi.nlm.nih.gov/biosystems) and Network2Canvas [@10.1093/bioinformatics/btt319], which is a LINCS-BD2K project.
<br>
</td>

                    <td class="" style="width: 35px;" colspan="3"></td>
                    <td class="diff-context" style="width: 45%;">
<br>It is difficult to estimate the total number of new human gene symbols in the entire corpus of published pathway images. The proportion of new genes must plateau—new results giving diminishing returns—as the total number of known genes in pathways is approached. But for perspective, even if we were to only consider the first 3985 images and conservatively estimate the average number of genes per pathway image to be 7, and generously estimate the false positive rate to be as high as 20%, then at a proportion of 34% we would expect ~6100 new genes. **This would almost double the count of unique human genes at WikiPathways today—the equivalent of 6 years of work at current crowdsourcing rates.** The effort would also greatly expand upon the interactions and biological context for practically all the genes in WikiPathways. As detailed in our data sharing plan, all of this new knowledge will be available in multiple formats, including BioPAX and RDF, and distributed to a wide range of independent resources through channels already established by the WikiPathways project, including Pathway Commons [@10.1093/nar/gkq1039], [NCBI](www.ncbi.nlm.nih.gov/biosystems) and Network2Canvas [@10.1093/bioinformatics/btt319], which is a LINCS-BD2K project.
<br>
</td>
                </tr>
            
            
            

            
            

            
            
        

            
            
            
            
            
            
            
            
                <tr class="repo-line" height="25px">
                    <td class="" style="width: 35px;" colspan="8"><hr></td>
                </tr>
            

            
            

            
            
        

            
            
            
            
            
            
                <tr class="repo-line" height="25px">
                    <td class="" style="width: 35px;" colspan="3"></td>
                    <td class="diff-context" style="width: 45%;">### Aim 1: Collect, Process, and Classify Pathway Images
<br>We implemented an efficient process to maximize collection of pathway images from PMC publication figures as part of the sample analysis described above. The process starts with a query into the PMC image search feature. Results are returned as HTML, which is parsed to download the full-size figure image files and generate an annotation file for each image. The annotation file, containing figure caption, author names, article title, and article hyperlink, will allow us to present critical contextual information during the crowdsourcing stage. It will also be used to index images by author and to support focused keyword searches (e.g., for images relating to particular diseases). This process will scale to encompass a broad range of queries to collect diverse and high-value pathway image file sets. We plan to collect a set of 16,000 pathway images in the first iteration of Aim 1, approximately 4x the size of the sample set described above.
<br>
</td>

                    <td class="" style="width: 35px;" colspan="3"></td>
                    <td class="diff-context" style="width: 45%;">### Aim 1: Collect, Process, and Classify Pathway Images
<br>We implemented an efficient process to maximize collection of pathway images from PMC publication figures as part of the sample analysis described above. The process starts with a query into the PMC image search feature. Results are returned as HTML, which is parsed to download the full-size figure image files and generate an annotation file for each image. The annotation file, containing figure caption, author names, article title, and article hyperlink, will allow us to present critical contextual information during the crowdsourcing stage. It will also be used to index images by author and to support focused keyword searches (e.g., for images relating to particular diseases). This process will scale to encompass a broad range of queries to collect diverse and high-value pathway image file sets. We plan to collect a set of 16,000 pathway images in the first iteration of Aim 1, approximately 4x the size of the sample set described above.
<br>
</td>
                </tr>
            
            
            

            
            

            
            
        

            
            
            
            
            
            
            
            

            
            
                <tr class="repo-line">
                    <td class="diff-marker" style="width: 35px;" colspan="3">-</td>
                    <td class="sub-line diff-context" style="width: 45%;">Also as part of the sample analysis, we began to explore text-extraction software. We have already scripted the application of Adobe Acrobat and Tesseract to generate extracted text file sets. These are useful for assessing the results in terms of recognizable gene symbol counts. But these programs also provide positional information for each block of extracted text. We will use this information to generate JSON models of nodes, preserving the annotation and position from the original image. This will allow us to overlay an interactive layer of modeled nodes onto the original pathway figures (see Aim 2). We will also improve the out-of-the-box performance of these programs. As an actively developed, open-source effort, Tesseract, in particular has considerable potential for improvement. To both programs, we will add a common image pre-processing step that uses ImageMagick (www.imagemagick.org) to increase contrast, adjust orientation, and remove noise (e.g., other graphics) from a copy of the image. We will also use OpenCV [@10.1145/2184319.2184337] to identify and optimize regions of text in the image prior to OCR. Other methods are available to isolate, orient, and filter “objects” that may contain recognizable text [@10.1093/bioinformatics/bts018 @10.1093/bioinformatics/btp318 @10.1109/bsec.2011.5872319 @10.1186/2041-1480-5-10]. This is a worthwhile area to explore, considering the Difficulty Matrix on our sample set of 3985 images ([Figure](#DifficultyMatrix)). Fortunately, the Easy-Easy corner of the matrix (top left) contains a large proportion of pathway figure images, which can be targeted in early rounds of our crowdsourcing effort. However, half of the images are in the Easy-Hard quadrant (easy for human, hard for computer; top right), meaning that any improvements in automatic text extraction will result in having more pathways that are ready and amenable for human processing. The lower half of the matrix includes such small percentages that we can simply ignore it for the purposes of this proposal. Of course, the difficulty level is not really binary, so we can leverage the gradient of difficulty in the human scale, for example, to rank the pathways for display to a gradient of participants from novice to expert.
</td>
            

            
            
        

            
            
            
                <tr class="repo-line" height="25px">
                    <td style="width: 35px;" colspan="4"></td>
            
            
            
            
            
            

            
            

            
            
        

            
            
            
            
            
            
            
            

            
            

            
            
                    <td class="diff-marker" style="width: 35px;" colspan="3">+</td>
                    <td class="add-line diff-context" style="width: 45%;">Also as part of the sample analysis, we began to explore text-extraction software. We have already scripted the application of Adobe Acrobat and Tesseract to generate extracted text file sets. These are useful for assessing the results in terms of recognizable gene symbol counts. But these programs also provide positional information for each block of extracted text. We will use this information to generate JSON models of nodes, preserving the annotation and position from the original image. This will allow us to overlay an interactive layer of modeled nodes onto the original pathway figures (see Aim 2). We will also improve the out-of-the-box performance of these programs. As an actively developed, open-source effort, Tesseract, in particular has considerable potential for improvement. To both programs, we will add a common image pre-processing step that uses ImageMagick (www.imagemagick.org) to increase contrast, adjust orientation, and remove noise (e.g., other graphics) from a copy of the image. We will also use OpenCV [@10.1145/2184319.2184337] to identify and optimize regions of text in the image prior to OCR. Other methods are available to isolate, orient, and filter “objects” that may contain recognizable text [@10.1093/bioinformatics/bts018 @10.1093/bioinformatics/btp318 @10.1109/bsec.2011.5872319 @10.1186/2041-1480-5-10]. This is a worthwhile area to explore, considering the Difficulty Matrix on our sample set of 3985 images ([Figure {n}](#DifficultyMatrix)). Fortunately, the Easy-Easy corner of the matrix (top left) contains a large proportion of pathway figure images, which can be targeted in early rounds of our crowdsourcing effort. However, half of the images are in the Easy-Hard quadrant (easy for human, hard for computer; top right), meaning that any improvements in automatic text extraction will result in having more pathways that are ready and amenable for human processing. The lower half of the matrix includes such small percentages that we can simply ignore it for the purposes of this proposal. Of course, the difficulty level is not really binary, so we can leverage the gradient of difficulty in the human scale, for example, to rank the pathways for display to a gradient of participants from novice to expert.
</td>
                </tr>

            
        

            
            
            
            
            
            
                <tr class="repo-line" height="25px">
                    <td class="" style="width: 35px;" colspan="3"></td>
                    <td class="diff-context" style="width: 45%;">
<br>[:figure](DifficultyMatrix)
<br>
</td>

                    <td class="" style="width: 35px;" colspan="3"></td>
                    <td class="diff-context" style="width: 45%;">
<br>[:figure](DifficultyMatrix)
<br>
</td>
                </tr>
            
            
            

            
            

            
            
        

            
            
            
            
            
            
            
            

            
            
                <tr class="repo-line">
                    <td class="diff-marker" style="width: 35px;" colspan="3">-</td>
                    <td class="sub-line diff-context" style="width: 45%;">The independent difficulty levels for humans and computers are just one example of how images will be classified. We will also assess the potential gene content per image based on the automatically extracted text. As we did with the sample set, we will contrast these gene sets with those already captured in properly modeled pathways. Given the set of novel genes, we will classify images based on the proportion and absolute number each pathway represents ([Figure](#HGNC)B, green). This will allow us to define high-value targets for the crowdsourcing effort—those images that will add the most new unique genes at the highest rate to pathway knowledge bases. We will also classify pathway images by the overrepresented GO terms and disease associations in their gene sets. Classification by disease, for example, will allow us to prioritize not only generally but also specifically for crowdsourcing efforts that target a single disease or research area. Even in cases where most of the genes are not novel, the interactions and contextual information that will be modeled are just as likely to impact subsequent research.
</td>
            

            
            
        

            
            
            
                <tr class="repo-line" height="25px">
                    <td style="width: 35px;" colspan="4"></td>
            
            
            
            
            
            

            
            

            
            
        

            
            
            
            
            
            
            
            

            
            

            
            
                    <td class="diff-marker" style="width: 35px;" colspan="3">+</td>
                    <td class="add-line diff-context" style="width: 45%;">The independent difficulty levels for humans and computers are just one example of how images will be classified. We will also assess the potential gene content per image based on the automatically extracted text. As we did with the sample set, we will contrast these gene sets with those already captured in properly modeled pathways. Given the set of novel genes, we will classify images based on the proportion and absolute number each pathway represents ([Figure {n}B, green](#HGNC)). This will allow us to define high-value targets for the crowdsourcing effort—those images that will add the most new unique genes at the highest rate to pathway knowledge bases. We will also classify pathway images by the overrepresented GO terms and disease associations in their gene sets. Classification by disease, for example, will allow us to prioritize not only generally but also specifically for crowdsourcing efforts that target a single disease or research area. Even in cases where most of the genes are not novel, the interactions and contextual information that will be modeled are just as likely to impact subsequent research.
</td>
                </tr>

            
        

            
            
            
            
            
            
                <tr class="repo-line" height="25px">
                    <td class="" style="width: 35px;" colspan="3"></td>
                    <td class="diff-context" style="width: 45%;">
<br>The product of this aim is an ever-growing database of pathway images, annotated not only with source information but also with extracted gene symbols, multiple dimensions of functional classifications, and a JSON data overlay ready to be rendered and made interactive in Aim 2.
<br>
</td>

                    <td class="" style="width: 35px;" colspan="3"></td>
                    <td class="diff-context" style="width: 45%;">
<br>The product of this aim is an ever-growing database of pathway images, annotated not only with source information but also with extracted gene symbols, multiple dimensions of functional classifications, and a JSON data overlay ready to be rendered and made interactive in Aim 2.
<br>
</td>
                </tr>
            
            
            

            
            

            
            
        

            
            
            
            
            
            
            
            
                <tr class="repo-line" height="25px">
                    <td class="" style="width: 35px;" colspan="8"><hr></td>
                </tr>
            

            
            

            
            
        

            
            
            
            
            
            
                <tr class="repo-line" height="25px">
                    <td class="" style="width: 35px;" colspan="3"></td>
                    <td class="diff-context" style="width: 45%;">
<br>We have sufficient infrastructure in place to develop and test the platform. As a modular set of virtualized services, we will deploy them using Amazon Web Services (AWS) to host large-scale beta and production crowdsourcing events toward the end of the funding period. By then we will have demonstrated the viability, scalability, and initial popularity of our approach, which will inform the strategy plan in future iterations.
<br>
</td>

                    <td class="" style="width: 35px;" colspan="3"></td>
                    <td class="diff-context" style="width: 45%;">
<br>We have sufficient infrastructure in place to develop and test the platform. As a modular set of virtualized services, we will deploy them using Amazon Web Services (AWS) to host large-scale beta and production crowdsourcing events toward the end of the funding period. By then we will have demonstrated the viability, scalability, and initial popularity of our approach, which will inform the strategy plan in future iterations.
<br>
</td>
                </tr>
            
            
            

            
            

            
            
        

            
            
            
            
            
            
            
            

            
            
                <tr class="repo-line">
                    <td class="diff-marker" style="width: 35px;" colspan="3">-</td>
                    <td class="sub-line diff-context" style="width: 45%;">**Customized Pvjs**—Pvjs will require customization to work as a component of the Pathways4Life platform. The modular architecture of pvjs will readily accommodate customization. The new modules will add support for attribute-value accessory data and an SVG visual feedback layer ([Figure](#Interface)).
</td>
            

            
            
        

            
            
            
                <tr class="repo-line" height="25px">
                    <td style="width: 35px;" colspan="4"></td>
            
            
            
            
            
            

            
            

            
            
        

            
            
            
            
            
            
            
            

            
            

            
            
                    <td class="diff-marker" style="width: 35px;" colspan="3">+</td>
                    <td class="add-line diff-context" style="width: 45%;">**Customized Pvjs**—Pvjs will require customization to work as a component of the Pathways4Life platform. The modular architecture of pvjs will readily accommodate customization. The new modules will add support for attribute-value accessory data and an SVG visual feedback layer ([Figure {n}](#Interface)).
</td>
                </tr>

            
        

            
            
            
            
            
            
                <tr class="repo-line" height="25px">
                    <td class="" style="width: 35px;" colspan="3"></td>
                    <td class="diff-context" style="width: 45%;"> 
<br>[:figure](Interface)
<br>
</td>

                    <td class="" style="width: 35px;" colspan="3"></td>
                    <td class="diff-context" style="width: 45%;"> 
<br>[:figure](Interface)
<br>
</td>
                </tr>
            
            
            

            
            

            
            
        

            
            
            
            
            
            
            
            
                <tr class="repo-line" height="25px">
                    <td class="" style="width: 35px;" colspan="8"><hr></td>
                </tr>
            

            
            

            
            
        

            
            
            
            
            
            
                <tr class="repo-line" height="25px">
                    <td class="" style="width: 35px;" colspan="3"></td>
                    <td class="diff-context" style="width: 45%;">### Aim 3: Crowdsource Tasks and Engage Participation
<br>Participants will define nodes and interactions. To define an interaction, the participant clicks on an existing node to anchor the source (an active “rubber band” line will now track with the mouse position) and then clicks on a second node or another interaction to indicate the target (an interaction arrow will now be drawn). A list of interaction types will appear from which the participant must select to complete the task and move on. To define a node, the participant right-clicks on the image where the node should be added (e.g., on the name or symbol for a gene, protein, or metabolite that OCR failed to recognize), types the name or symbol (which triggers an autocomplete pvjs database lookup), and then selects the correct identity (a new node will then be drawn). Subsequent interactions to and from the newly added nodes can then be drawn. In this way, complete pathway images can be traced and effectively modeled by a series of these two easy-to-learn tasks.
<br>
</td>

                    <td class="" style="width: 35px;" colspan="3"></td>
                    <td class="diff-context" style="width: 45%;">### Aim 3: Crowdsource Tasks and Engage Participation
<br>Participants will define nodes and interactions. To define an interaction, the participant clicks on an existing node to anchor the source (an active “rubber band” line will now track with the mouse position) and then clicks on a second node or another interaction to indicate the target (an interaction arrow will now be drawn). A list of interaction types will appear from which the participant must select to complete the task and move on. To define a node, the participant right-clicks on the image where the node should be added (e.g., on the name or symbol for a gene, protein, or metabolite that OCR failed to recognize), types the name or symbol (which triggers an autocomplete pvjs database lookup), and then selects the correct identity (a new node will then be drawn). Subsequent interactions to and from the newly added nodes can then be drawn. In this way, complete pathway images can be traced and effectively modeled by a series of these two easy-to-learn tasks.
<br>
</td>
                </tr>
            
            
            

            
            

            
            
        

            
            
            
            
            
            
            
            

            
            
                <tr class="repo-line">
                    <td class="diff-marker" style="width: 35px;" colspan="3">-</td>
                    <td class="sub-line diff-context" style="width: 45%;">Each task will be associated with an adjustable point value. Tunable point values support the basic game mechanic of balancing the economy of player’s attention and time investment. For example, rare nodes and interactions will be worth more points than common ones already captured in the current archives of pathway models. This will allow us to tune the prioritization of novel information. The overall difficulty of a given pathway (i.e., per human difficulty scale in [Figure](#DifficultyMatrix)) can also be a variable in calculating a task’s value, both to balance challenge and reward and to encourage skill building and return participation. Even the ordinality (1st...Nth) could be used to value nodes and interactions to encourage completion of a given pathway.
</td>
            

            
            
        

            
            
            
                <tr class="repo-line" height="25px">
                    <td style="width: 35px;" colspan="4"></td>
            
            
            
            
            
            

            
            

            
            
        

            
            
            
            
            
            
            
            

            
            

            
            
                    <td class="diff-marker" style="width: 35px;" colspan="3">+</td>
                    <td class="add-line diff-context" style="width: 45%;">Each task will be associated with an adjustable point value. Tunable point values support the basic game mechanic of balancing the economy of player’s attention and time investment. For example, rare nodes and interactions will be worth more points than common ones already captured in the current archives of pathway models. This will allow us to tune the prioritization of novel information. The overall difficulty of a given pathway (i.e., per human difficulty scale in [Figure {n}](#DifficultyMatrix)) can also be a variable in calculating a task’s value, both to balance challenge and reward and to encourage skill building and return participation. Even the ordinality (1st...Nth) could be used to value nodes and interactions to encourage completion of a given pathway.
</td>
                </tr>

            
        

            
            
            
            
            
            
                <tr class="repo-line" height="25px">
                    <td class="" style="width: 35px;" colspan="3"></td>
                    <td class="diff-context" style="width: 45%;">
</td>

                    <td class="" style="width: 35px;" colspan="3"></td>
                    <td class="diff-context" style="width: 45%;">
</td>
                </tr>
            
            
            

            
            

            
            
        

            
            
            
            
            
            
            
            

            
            
                <tr class="repo-line">
                    <td class="diff-marker" style="width: 35px;" colspan="3">-</td>
                    <td class="sub-line diff-context" style="width: 45%;">To assess quality and confidence (see Aim 4), we will need to collect redundant information from multiple participants on any given task. We will do this in two ways: (1) by showing the same version of a pathway to multiple participants, excluding newly added nodes and interactions that have yet to be confirmed and (2) by allowing participants to right-click on existing nodes and interactions to contest the information, which would contribute to a confirmed rejection and removal of that information in future rounds. This strategy will also help address false-positive OCR results that generate inaccurate nodes. Again, tunable point values will be used to balance confirmation versus pioneering activity (e.g., by increasing the values for successive confirmations). And participants will gain/lose points post hoc based on the long-term confirmation/rejection status of their tasks. This tunable value will balance accuracy against speed. [Table](#TunableVariables) summarizes the tunable economy of the platform via task point values, as well as when the calculation occurs.
</td>
            

            
            
        

            
            
            
                <tr class="repo-line" height="25px">
                    <td style="width: 35px;" colspan="4"></td>
            
            
            
            
            
            

            
            

            
            
        

            
            
            
            
            
            
            
            

            
            

            
            
                    <td class="diff-marker" style="width: 35px;" colspan="3">+</td>
                    <td class="add-line diff-context" style="width: 45%;">To assess quality and confidence (see Aim 4), we will need to collect redundant information from multiple participants on any given task. We will do this in two ways: (1) by showing the same version of a pathway to multiple participants, excluding newly added nodes and interactions that have yet to be confirmed and (2) by allowing participants to right-click on existing nodes and interactions to contest the information, which would contribute to a confirmed rejection and removal of that information in future rounds. This strategy will also help address false-positive OCR results that generate inaccurate nodes. Again, tunable point values will be used to balance confirmation versus pioneering activity (e.g., by increasing the values for successive confirmations). And participants will gain/lose points post hoc based on the long-term confirmation/rejection status of their tasks. This tunable value will balance accuracy against speed. [Table {n}](#TunableVariables) summarizes the tunable economy of the platform via task point values, as well as when the calculation occurs.
</td>
                </tr>

            
        

            
            
            
            
            
            
                <tr class="repo-line" height="25px">
                    <td class="" style="width: 35px;" colspan="3"></td>
                    <td class="diff-context" style="width: 45%;">
<br>[:table](TunableVariables)
<br>
</td>

                    <td class="" style="width: 35px;" colspan="3"></td>
                    <td class="diff-context" style="width: 45%;">
<br>[:table](TunableVariables)
<br>
</td>
                </tr>
            
            
            

            
            

            
            
        

            
            
            
            
            
            
            
            
                <tr class="repo-line" height="25px">
                    <td class="" style="width: 35px;" colspan="8"><hr></td>
                </tr>
            

            
            

            
            
        
        </table>

    



		</div>


		
	    	<div class="footer">
<div style="background:#dedede;">
	<div class="main-centered-div text-center" style="padding:20px;">
		<h4 style="margin-bottom:0px;font-weight:bold;color:gray;">Feedback? Questions? Email us: <a href="/cdn-cgi/l/email-protection#7c14191010133c0814151217101d1e521f1311"><span class="__cf_email__" data-cfemail="0d65686161624d7965646366616c6f236e6260">[email&#160;protected]</span><script data-cfhash='f9e31' type="text/javascript">/* <![CDATA[ */!function(t,e,r,n,c,a,p){try{t=document.currentScript||function(){for(t=document.getElementsByTagName('script'),e=t.length;e--;)if(t[e].getAttribute('data-cfhash'))return t[e]}();if(t&&(c=t.previousSibling)){p=t.parentNode;if(a=c.getAttribute('data-cfemail')){for(e='',r='0x'+a.substr(0,2)|0,n=2;a.length-n;n+=2)e+='%'+('0'+('0x'+a.substr(n,2)^r).toString(16)).slice(-2);p.replaceChild(document.createTextNode(decodeURIComponent(e)),c)}p.removeChild(t)}}catch(u){}}()/* ]]> */</script></a></h4>
	</div>
</div>
<div class="main-footer">
	<div class="main-centered-div">

<table class="full-width"><tr>
	<td class="footer-nav">

        <div class="foot-head">Explore</div>
        <a href="/proposals">Proposals</a><br>
        <a href="/projects">Projects</a><br>
        <a href="/publications">Article discussion</a><br>
        <a href="/discussion">All discussion</a><br>
        <a href="/topics">Topics</a><br>
        <!-- <a href="/funders">Funders</a><br> -->
        <a href="/funding">Funding opportunities</a><br>

	</td>
	<td class="footer-nav">

        <!-- <div class="foot-head">Help &amp; feedback</div>	 -->
        <!-- <div class="foot-head">Intro</div>
        <a href="/benefits/for-science">Why</a><br>
        <a href="/how-it-works">How it works</a><br>

<br> -->

        <div class="foot-head">Community</div>
        <a href="/leaderboard">Leaderboard</a><br>
        <a href="/p/meta">Thinklab Meta</a><br>
				<a href="/help/faq">Help</a><br>
				<!-- <a href="/new-project">Start a proposal</a><br> -->
        <!-- <a href="/benefits/proposal">Benefits</a><br> -->
        <!-- <a href="/funding">Get funded</a><br> -->
        <!-- <a href="/participate">Become a contributor</a><br> -->
        <!-- <a href="/for-funders">For science funders</a><br> -->
        <!-- <a href="/for-institutions">For institutions</a><br> -->

	</td>
	<td class="footer-nav">



        <div class="foot-head">About</div>
        <a href="/about">Overview</a><br>
        <a href="/benefits">Benefits</a><br>
        <a href="/how-it-works">How it works</a><br>
        <a href="/about/story">Our story</a><br>
        <a href="/blog">Our blog</a><br>
        <a href="/about/jobs">Join the team</a><br>
        <br>

        <!-- <div class="foot-head">Help</div>
        <a href="/help">Help</a><br>
		 -->


	</td>
	<td class="footer-nav">
		<a href="http://twitter.com/thinklab" class="social-icon"><img src="https://think-lab.s3.amazonaws.com/s/img/icons/twitter_com.png" width="40"/></a>
		<a href="https://www.facebook.com/pages/Thinklab/310895039088185" class="social-icon"><img src="https://think-lab.s3.amazonaws.com/s/img/icons/facebook_com.png" width="40"/></a>
		<a href="https://plus.google.com/115578983466800753402/" class="social-icon"><img src="https://think-lab.s3.amazonaws.com/s/img/icons/plus_google_com.png" width="40"/></a>
		<a href="https://www.linkedin.com/company/thinklab-co" class="social-icon"><img src="https://think-lab.s3.amazonaws.com/s/img/icons/linkedin_com.png" width="40"/></a>
		<br><br>
		<span class="light" style='font-size:12px;'>&copy; 2015 Thinklab</span>

	</td>

</tr></table>

</div>
</div>
</div>

		

	</div>


  <script></script>
  <script>
  var MEDIA_URL = 'https://think-lab.s3.amazonaws.com/m/';
  var STATIC_URL = 'https://think-lab.s3.amazonaws.com/s/';
  var page_is_authenticated = false;
  var page_page = '';
  var page_profile_id = undefined;
</script>



	<!-- Note different script type. MUST come before MathJax.js -->
	<script type="text/x-mathjax-config">
	    MathJax.Hub.Config({
	        "HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"],
	                        scale: 110, linebreaks: { automatic:true },
	                        EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
	        tex2jax: { inlineMath: [ ["$$", "$$"], ["\\(","\\)"] ],
	                    displayMath: [ ["$$$","$$$"], ["\\[", "\\]"] ],
	                    processEscapes: true,}
	            });

	</script>
    <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>





  <script src="https://code.jquery.com/jquery-1.10.2.min.js"></script>
  <script src="https://code.jquery.com/ui/1.11.4/jquery-ui.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/js/bootstrap.min.js" integrity="sha384-0mSbJDEHialfmuBBQP6A4Qrprq5OVfW37PRR3j5ELqxss1yVqOtnepnHVP9aJ7xS" crossorigin="anonymous"></script>
  <script src="https://think-lab.s3.amazonaws.com/s/dist/main.min.js"></script>
  



  
	
<script>/* <![CDATA[ */(function(d,s,a,i,j,r,l,m,t){try{l=d.getElementsByTagName('a');t=d.createElement('textarea');for(i=0;l.length-i;i++){try{a=l[i].href;s=a.indexOf('/cdn-cgi/l/email-protection');m=a.length;if(a&&s>-1&&m>28){j=28+s;s='';if(j<m){r='0x'+a.substr(j,2)|0;for(j+=2;j<m&&a.charAt(j)!='X';j+=2)s+='%'+('0'+('0x'+a.substr(j,2)^r).toString(16)).slice(-2);j++;s=decodeURIComponent(s)+a.substr(j,m-j)}t.innerHTML=s.replace(/</g,'&lt;').replace(/\>/g,'&gt;');l[i].href='mailto:'+t.value}}catch(e){}}}catch(e){}})(document);/* ]]> */</script></body>
</html>
